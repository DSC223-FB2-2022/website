---
format: revealjs
---


# Models with multiple predictors


```{r echo = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(patchwork)
set.seed(1234)
options(dplyr.print_min = 10, dplyr.print_max = 6)
```


## Data: Book weight and volume {.smaller}

The `allbacks` data frame gives measurements on the volume and weight of 15 books, some of which are paperback and some of which are hardback

:::: {.columns}

::: {.column width="50%"}

- Volume - cubic centimetres
- Area - square centimetres
- Weight - grams
:::

::: {.column width="50%"}


```{r echo=FALSE}
library(DAAG)
as_tibble(allbacks) %>%
  print(n = 15)
```

:::

::::


::: aside
These books are from the bookshelf of J. H. Maindonald at Australian National University.
:::

## Book weight vs. volume {.smaller}    

:::: {.columns}

::: {.column width="60%"}


```{r}
linear_reg() %>%
  set_engine("lm") %>%
  fit(weight ~ volume, data = allbacks) %>%
  tidy()
```

:::

::: {.column width="40%"}


```{r out.width = "75%", echo = FALSE, fig.align = "right"}
ggplot(allbacks, aes(x = volume, y = weight)) +
  geom_point(alpha = 0.7, size = 3)
```

:::

::::

## Book weight vs. volume and cover {.smaller}

:::: {.columns}

::: {.column width="60%"}


```{r}
linear_reg() %>%
  set_engine("lm") %>%
  fit(weight ~ volume + cover, data = allbacks) %>%
  tidy()
```

:::

::: {.column width="40%"}


```{r out.width = "75%", echo = FALSE, fig.align = "right"}
ggplot(allbacks, aes(x = volume, y = weight, color = cover, shape = cover)) +
  geom_point(alpha = 0.7, size = 3) +
  theme(legend.position = "bottom") +
  scale_color_manual(values = c("#E48957", "#071381"))
```

:::

::::

## Interpretation of estimates {.incremental .smaller}


```{r echo=FALSE}
linear_reg() %>%
  set_engine("lm") %>%
  fit(weight ~ volume + cover, data = allbacks) %>%
  tidy()
```

- **Slope - volume:** *All else held constant*, for each additional cubic centimetre books are larger in volume, we would expect the weight to be higher, on average, by 0.718 grams.
- **Slope - cover:** *All else held constant*, paperback books are weigh, on average, by 184 grams less than hardcover books.
- **Intercept:** Hardcover books with 0 volume are expected to weigh 198 grams, on average. (Doesn't make sense in context.)

## Main vs. interaction effects 

***Suppose we want to predict weight of books from their volume and cover type (hardback vs. paperback). Do you think a model with main effects or interaction effects is more appropriate? Explain your reasoning.***

**Hint:** Main effects would mean rate at which weight changes as volume 
increases would be the same for hardback and paperback books and interaction 
effects would mean the rate at which weight changes as volume 
increases would be different for hardback and paperback books.


##  {data-menu-title="Interaction Graph Example"}


```{r book-main-int, echo=FALSE, out.width="65%", fig.asp = 0.8}
book_main_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(weight ~ volume + cover, data = allbacks)
book_main_fit_aug <- augment(book_main_fit$fit)

book_int_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(weight ~ volume + cover + volume*cover, data = allbacks)
book_int_fit_aug <- augment(book_int_fit$fit)

p_main <- ggplot() +
  geom_point(book_main_fit_aug, 
             mapping = aes(x = volume, y = weight, color = cover, shape = cover), alpha = 0.7) +
  geom_line(book_main_fit_aug, 
            mapping = aes(x = volume, y = .fitted, color = cover)) +
  labs(title = "Main effects, parallel slopes", 
       subtitle = "weight-hat = volume + cover") +
  scale_color_manual(values = c("#E48957", "#071381"))

p_int <- ggplot() +
  geom_point(book_int_fit_aug, 
             mapping = aes(x = volume, y = weight, color = cover, shape = cover), alpha = 0.7) +
  geom_line(book_int_fit_aug, 
            mapping = aes(x = volume, y = .fitted, color = cover)) +
  labs(title = "Interaction effects, not parallel slopes", 
       subtitle = "weight-hat = volume + cover + volume * cover") +
  scale_color_manual(values = c("#E48957", "#071381"))

p_main /
  p_int +
  plot_layout(guides = "collect")
```


## In pursuit of Occam's razor {.incremental}

- Occam's Razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected.
- Model selection follows this principle.
- We only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model.
- In other words, we prefer the simplest best model, i.e. **parsimonious** model.

##  {data-menu-title="Occam's razor" .smaller}

:::: {.columns}

::: {.column width="75%"}


```{r ref.label = "book-main-int", echo = FALSE, out.width="100%", fig.asp = 0.8}
```

:::

::: {.column width="25%"}

***Visually, which of the two models is preferable under Occam's razor?***
:::

::::

## R-squared {.incremental}

- $R^2$ is the percentage of variability in the response variable explained by 
the regression model.


```{r}
glance(book_main_fit)$r.squared
glance(book_int_fit)$r.squared
```


- Clearly the model with interactions has a higher $R^2$.

- However using $R^2$ for model selection in models with multiple explanatory variables is not a good idea as $R^2$ increases when **any** variable is added to the model.

## Adjusted R-squared

... a (more) objective measure for model selection

- Adjusted $R^2$ doesn't increase if the new variable does not provide any new 
informaton or is completely unrelated, as it applies a penalty for number of 
variables included in the model.
- This makes adjusted $R^2$ a preferable metric for model selection in multiple
regression models.

## Comparing models {.incremental .smaller}

:::: {.columns}

::: {.column width="50%"}


```{r}
glance(book_main_fit)$r.squared
glance(book_int_fit)$r.squared
```

:::

::: {.column width="50%"}


```{r}
glance(book_main_fit)$adj.r.squared
glance(book_int_fit)$adj.r.squared
```

:::

::::


- Is R-sq higher for int model?


```{r}
glance(book_int_fit)$r.squared > glance(book_main_fit)$r.squared 
```


- Is R-sq adj. higher for int model?


```{r}
glance(book_int_fit)$adj.r.squared > glance(book_main_fit)$adj.r.squared
```

