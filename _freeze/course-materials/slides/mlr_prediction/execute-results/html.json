{
  "hash": "4c9a4f0cd68297e1cd2eda1c05f03da0",
  "result": {
    "markdown": "---\ntitle: \"Multiple Linear Regression, Prediction, and Overfitting\"\nsubtitle: \"<br><br> Data Science in a Box\"\nauthor: \"[datasciencebox.org](https://datasciencebox.org/)\"\ndate: \"October 13th, 2022\"\nformat: revealjs\n---\n---\nformat: revealjs\n---\n\n# Models with multiple predictors\n\n::: {.cell}\n\n:::\n\n## Data: Book weight and volume {.smaller}\n\nThe `allbacks` data frame gives measurements on the volume and weight of 15 books, some of which are paperback and some of which are hardback\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n- Volume - cubic centimetres\n- Area - square centimetres\n- Weight - grams\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 15 x 4\n   volume  area weight cover\n    <dbl> <dbl>  <dbl> <fct>\n 1    885   382    800 hb   \n 2   1016   468    950 hb   \n 3   1125   387   1050 hb   \n 4    239   371    350 hb   \n 5    701   371    750 hb   \n 6    641   367    600 hb   \n 7   1228   396   1075 hb   \n 8    412     0    250 pb   \n 9    953     0    700 pb   \n10    929     0    650 pb   \n11   1492     0    975 pb   \n12    419     0    350 pb   \n13   1010     0    950 pb   \n14    595     0    425 pb   \n15   1034     0    725 pb   \n```\n:::\n:::\n:::\n\n::::\n\n\n::: aside\nThese books are from the bookshelf of J. H. Maindonald at Australian National University.\n:::\n\n## Book weight vs. volume {.smaller}    \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(weight ~ volume, data = allbacks) %>%\n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 5\n  term        estimate std.error statistic    p.value\n  <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)  108.      88.4         1.22 0.245     \n2 volume         0.709    0.0975      7.27 0.00000626\n```\n:::\n:::\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell layout-align=\"right\"}\n::: {.cell-output-display}\n![](mlr_prediction_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='right' width=75%}\n:::\n:::\n:::\n\n::::\n\n## Book weight vs. volume and cover {.smaller}\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(weight ~ volume + cover, data = allbacks) %>%\n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 5\n  term        estimate std.error statistic      p.value\n  <chr>          <dbl>     <dbl>     <dbl>        <dbl>\n1 (Intercept)  198.      59.2         3.34 0.00584     \n2 volume         0.718    0.0615     11.7  0.0000000660\n3 coverpb     -184.      40.5        -4.55 0.000672    \n```\n:::\n:::\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell layout-align=\"right\"}\n::: {.cell-output-display}\n![](mlr_prediction_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='right' width=75%}\n:::\n:::\n:::\n\n::::\n\n## Interpretation of estimates {.incremental .smaller}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 5\n  term        estimate std.error statistic      p.value\n  <chr>          <dbl>     <dbl>     <dbl>        <dbl>\n1 (Intercept)  198.      59.2         3.34 0.00584     \n2 volume         0.718    0.0615     11.7  0.0000000660\n3 coverpb     -184.      40.5        -4.55 0.000672    \n```\n:::\n:::\n- **Slope - volume:** *All else held constant*, for each additional cubic centimetre books are larger in volume, we would expect the weight to be higher, on average, by 0.718 grams.\n- **Slope - cover:** *All else held constant*, paperback books are weigh, on average, by 184 grams less than hardcover books.\n- **Intercept:** Hardcover books with 0 volume are expected to weigh 198 grams, on average. (Doesn't make sense in context.)\n\n## Main vs. interaction effects \n\n***Suppose we want to predict weight of books from their volume and cover type (hardback vs. paperback). Do you think a model with main effects or interaction effects is more appropriate? Explain your reasoning.***\n\n**Hint:** Main effects would mean rate at which weight changes as volume \nincreases would be the same for hardback and paperback books and interaction \neffects would mean the rate at which weight changes as volume \nincreases would be different for hardback and paperback books.\n\n\n##  {data-menu-title=\"Interaction Graph Example\"}\n\n::: {.cell fig.asp='0.8'}\n::: {.cell-output-display}\n![](mlr_prediction_files/figure-revealjs/book-main-int-1.png){width=65%}\n:::\n:::\n\n## In pursuit of Occam's razor {.incremental}\n\n- Occam's Razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected.\n- Model selection follows this principle.\n- We only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model.\n- In other words, we prefer the simplest best model, i.e. **parsimonious** model.\n\n##  {data-menu-title=\"Occam's razor\" .smaller}\n\n:::: {.columns}\n\n::: {.column width=\"75%\"}\n\n::: {.cell fig.asp='0.8'}\n::: {.cell-output-display}\n![](mlr_prediction_files/figure-revealjs/unnamed-chunk-8-1.png){width=100%}\n:::\n:::\n:::\n\n::: {.column width=\"25%\"}\n\n***Visually, which of the two models is preferable under Occam's razor?***\n:::\n\n::::\n\n## R-squared {.incremental}\n\n- $R^2$ is the percentage of variability in the response variable explained by \nthe regression model.\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(book_main_fit)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9274776\n```\n:::\n\n```{.r .cell-code}\nglance(book_int_fit)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9297137\n```\n:::\n:::\n\n- Clearly the model with interactions has a higher $R^2$.\n\n- However using $R^2$ for model selection in models with multiple explanatory variables is not a good idea as $R^2$ increases when **any** variable is added to the model.\n\n## Adjusted R-squared\n\n... a (more) objective measure for model selection\n\n- Adjusted $R^2$ doesn't increase if the new variable does not provide any new \ninformaton or is completely unrelated, as it applies a penalty for number of \nvariables included in the model.\n- This makes adjusted $R^2$ a preferable metric for model selection in multiple\nregression models.\n\n## Comparing models {.incremental .smaller}\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(book_main_fit)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9274776\n```\n:::\n\n```{.r .cell-code}\nglance(book_int_fit)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9297137\n```\n:::\n:::\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(book_main_fit)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9153905\n```\n:::\n\n```{.r .cell-code}\nglance(book_int_fit)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9105447\n```\n:::\n:::\n:::\n\n::::\n\n\n- Is R-sq higher for int model?\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(book_int_fit)$r.squared > glance(book_main_fit)$r.squared \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n- Is R-sq adj. higher for int model?\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(book_int_fit)$adj.r.squared > glance(book_main_fit)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n:::\n\n\n\n---\nformat: revealjs\n---\n\n## Prediction and overfitting\n\n::: {.cell}\n\n:::\n\n## Prediction {.incremental .smaller}\n### Goal: Building a spam filter\n\n- Data: Set of emails and we know if each email is spam/not and other features \n- Use logistic regression to predict the probability that an incoming email is spam\n- Use model selection to pick the model with the best predictive performance\n\n- Building a model to predict the probability that an email is spam is only half of the battle! We also need a decision rule about which emails get flagged as spam (e.g. what probability should we use as out cutoff?)\n- A simple approach: choose a single threshold probability and any email that exceeds that probability is flagged as spam\n\n## A multiple regression approach {.smaller}\n\n::: {.panel-tabset}\n### output\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 22 x 5\n   term         estimate std.error statistic  p.value\n   <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)  -9.09e+1   9.80e+3  -0.00928 9.93e- 1\n 2 to_multiple1 -2.68e+0   3.27e-1  -8.21    2.25e-16\n 3 from1        -2.19e+1   9.80e+3  -0.00224 9.98e- 1\n 4 cc            1.88e-2   2.20e-2   0.855   3.93e- 1\n 5 sent_email1  -2.07e+1   3.87e+2  -0.0536  9.57e- 1\n 6 time          8.48e-8   2.85e-8   2.98    2.92e- 3\n 7 image        -1.78e+0   5.95e-1  -3.00    2.73e- 3\n 8 attach        7.35e-1   1.44e-1   5.09    3.61e- 7\n 9 dollar       -6.85e-2   2.64e-2  -2.59    9.64e- 3\n10 winneryes     2.07e+0   3.65e-1   5.67    1.41e- 8\n11 inherit       3.15e-1   1.56e-1   2.02    4.32e- 2\n12 viagra        2.84e+0   2.22e+3   0.00128 9.99e- 1\n13 password     -8.54e-1   2.97e-1  -2.88    4.03e- 3\n14 num_char      5.06e-2   2.38e-2   2.13    3.35e- 2\n15 line_breaks  -5.49e-3   1.35e-3  -4.06    4.91e- 5\n16 format1      -6.14e-1   1.49e-1  -4.14    3.53e- 5\n17 re_subj1     -1.64e+0   3.86e-1  -4.25    2.16e- 5\n18 exclaim_subj  1.42e-1   2.43e-1   0.585   5.58e- 1\n19 urgent_subj1  3.88e+0   1.32e+0   2.95    3.18e- 3\n20 exclaim_mess  1.08e-2   1.81e-3   5.98    2.23e- 9\n21 numbersmall  -1.19e+0   1.54e-1  -7.74    9.62e-15\n22 numberbig    -2.95e-1   2.20e-1  -1.34    1.79e- 1\n```\n:::\n:::\n\n### Code\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(spam ~ ., data = email, family = \"binomial\") %>%\n  tidy() %>%\n  print(n = 22)\n```\n:::\n:::\n\n## Prediction {.incremental}\n\n- The mechanics of prediction is **easy**:\n  - Plug in values of predictors to the model equation\n  - Calculate the predicted value of the response variable, $\\hat{y}$\n\n- Getting it right is **hard**!\n  - There is no guarantee the model estimates you have are correct\n  - Or that your model will perform as well with new data as it did with your sample data\n\n## Underfitting and overfitting\n\n::: {.cell}\n::: {.cell-output-display}\n![](mlr_prediction_files/figure-revealjs/unnamed-chunk-16-1.png){width=70%}\n:::\n:::\n\n## Spending our data\n\n- Several steps to create a useful model: parameter estimation, model selection, performance assessment, etc.\n\n- Doing all of this on the entire data we have available can lead to **overfitting**\n\n- Allocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only (which is what we've done so far)\n\n## Splitting data {.smaller}\n\n- **Training set:**\n  - Sandbox for model building \n  - Spend most of your time using the training set to develop the model\n  - Majority of the data (usually 80%)\n  \n- **Testing set:**\n  - Held in reserve to determine efficacy of one or two chosen models\n  - Critical to look at it once, otherwise it becomes part of the modeling process\n  - Remainder of the data (usually 20%)\n\n## Performing the split\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fix random numbers by setting the seed \n# Enables analysis to be reproducible when random numbers are used \nset.seed(1116)\n\n# Put 80% of the data into the training set \nemail_split <- initial_split(email, prop = 0.80)\n\n# Create data frames for the two sets:\ntrain_data <- training(email_split)\ntest_data  <- testing(email_split)\n```\n:::\n\n## Peek at the split {.smaller}\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(train_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 3,136\nColumns: 21\n$ spam         <fct> 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ to_multiple  <fct> 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,~\n$ from         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~\n$ cc           <int> 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 1, 0~\n$ sent_email   <fct> 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,~\n$ time         <dttm> 2012-01-25 16:46:55, 2012-01-02 23:28:28, 2012-02-04 10:31:11, 2012-03-19 08~\n$ image        <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ attach       <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ dollar       <dbl> 10, 0, 0, 0, 0, 0, 13, 0, 0, 0, 2, 0, 0, 0, 14, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,~\n$ winner       <fct> no, no, no, no, no, no, no, yes, no, no, no, no, no, no, no, no, no, no, no, ~\n$ inherit      <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ viagra       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ password     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ num_char     <dbl> 23.308, 1.162, 4.732, 42.238, 1.228, 25.599, 16.764, 10.731, 3.778, 27.182, 2~\n$ line_breaks  <int> 477, 2, 127, 712, 30, 674, 367, 226, 98, 671, 46, 192, 67, 85, 655, 18, 167, ~\n$ format       <fct> 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~\n$ re_subj      <fct> 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,~\n$ exclaim_subj <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,~\n$ urgent_subj  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ exclaim_mess <dbl> 12, 0, 2, 2, 2, 31, 2, 0, 0, 1, 0, 1, 2, 0, 2, 0, 11, 1, 4, 0, 1, 1, 3, 2, 0,~\n$ number       <fct> small, none, big, big, small, small, small, small, small, small, small, small~\n```\n:::\n:::\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(test_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 785\nColumns: 21\n$ spam         <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ to_multiple  <fct> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,~\n$ from         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~\n$ cc           <int> 0, 1, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 4, 0, 0, 0,~\n$ sent_email   <fct> 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,~\n$ time         <dttm> 2012-01-01 11:55:06, 2012-01-01 13:38:32, 2012-01-01 23:42:16, 2012-01-02 09~\n$ image        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ attach       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,~\n$ dollar       <dbl> 0, 0, 5, 0, 0, 0, 0, 5, 4, 0, 0, 0, 21, 0, 0, 2, 9, 0, 0, 0, 0, 20, 0, 0, 0, ~\n$ winner       <fct> no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, n~\n$ inherit      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ viagra       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ password     <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,~\n$ num_char     <dbl> 4.837, 15.075, 18.037, 45.842, 11.438, 1.482, 14.431, 0.978, 7.792, 0.978, 2.~\n$ line_breaks  <int> 193, 354, 345, 881, 125, 24, 296, 13, 192, 14, 32, 30, 557, 159, 81, 173, 151~\n$ format       <fct> 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,~\n$ re_subj      <fct> 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,~\n$ exclaim_subj <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ urgent_subj  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ exclaim_mess <dbl> 1, 10, 20, 5, 2, 0, 0, 0, 6, 0, 0, 1, 3, 0, 4, 0, 1, 0, 139, 2, 0, 18, 1, 8, ~\n$ number       <fct> big, small, small, big, small, none, small, small, small, small, small, none,~\n```\n:::\n:::\n:::\n\n::::\n\n## Modeling workflow\n\n## Fit a model to the training dataset\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_fit <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(spam ~ ., data = train_data, family = \"binomial\")\n```\n:::\n\n## Categorical predictors\n\n::: {.cell}\n::: {.cell-output-display}\n![](mlr_prediction_files/figure-revealjs/unnamed-chunk-21-1.png){width=75%}\n:::\n:::\n\n## `from` and `sent_email`\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n- `from`: Whether the message was listed as from anyone (this is usually set by default for regular outgoing email)\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_data %>%\n  count(spam, from)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 3\n  spam  from      n\n  <fct> <fct> <int>\n1 0     1      2837\n2 1     0         3\n3 1     1       296\n```\n:::\n:::\n:::\n\n::: {.column width=\"50%\"}\n\n- `sent_email`: Indicator for whether the sender had been sent an email in the last 30 days\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_data %>%\n  count(spam, sent_email)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 3\n  spam  sent_email     n\n  <fct> <fct>      <int>\n1 0     0           1972\n2 0     1            865\n3 1     0            299\n```\n:::\n:::\n:::\n\n::::\n\n\n## Numerical predictors {.smaller}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 22 x 12\n   skim_type skim_va~1 spam  n_mis~2 compl~3 numer~4 numer~5 numer~6 numer~7 numer~8 numer~9 numer~*\n   <chr>     <chr>     <fct>   <int>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 numeric   cc        0           0       1 0.393    2.62         0       0       0       0      68\n 2 numeric   cc        1           0       1 0.388    3.25         0       0       0       0      50\n 3 numeric   image     0           0       1 0.0536   0.503        0       0       0       0      20\n 4 numeric   image     1           0       1 0.00334  0.0578       0       0       0       0       1\n 5 numeric   attach    0           0       1 0.124    0.775        0       0       0       0      21\n 6 numeric   attach    1           0       1 0.227    0.620        0       0       0       0       2\n 7 numeric   dollar    0           0       1 1.56     5.33         0       0       0       0      64\n 8 numeric   dollar    1           0       1 0.779    3.01         0       0       0       0      36\n 9 numeric   inherit   0           0       1 0.0352   0.216        0       0       0       0       6\n10 numeric   inherit   1           0       1 0.0702   0.554        0       0       0       0       9\n# ... with 12 more rows, and abbreviated variable names 1: skim_variable, 2: n_missing,\n#   3: complete_rate, 4: numeric.mean, 5: numeric.sd, 6: numeric.p0, 7: numeric.p25,\n#   8: numeric.p50, 9: numeric.p75, *: numeric.p100\n```\n:::\n:::\n\n## Fit a model to the training dataset {.smaller}\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_fit <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(spam ~ . - from - sent_email - viagra, data = train_data, family = \"binomial\") #<<\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\n\nCall:  stats::glm(formula = spam ~ . - from - sent_email - viagra, family = stats::binomial, \n    data = data)\n\nCoefficients:\n (Intercept)  to_multiple1            cc          time         image        attach        dollar  \n  -9.867e+01    -2.505e+00     1.944e-02     7.396e-08    -2.854e+00     5.070e-01    -6.440e-02  \n   winneryes       inherit      password      num_char   line_breaks       format1      re_subj1  \n   2.170e+00     4.499e-01    -7.065e-01     5.870e-02    -5.420e-03    -9.017e-01    -2.995e+00  \nexclaim_subj  urgent_subj1  exclaim_mess   numbersmall     numberbig  \n   1.002e-01     3.572e+00     1.009e-02    -8.518e-01    -1.329e-01  \n\nDegrees of Freedom: 3135 Total (i.e. Null);  3117 Residual\nNull Deviance:\t    1974 \nResidual Deviance: 1447 \tAIC: 1485\n```\n:::\n:::\n\n## Predict outcome on the testing dataset\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(email_fit, test_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 785 x 1\n   .pred_class\n   <fct>      \n 1 0          \n 2 0          \n 3 0          \n 4 0          \n 5 0          \n 6 0          \n 7 0          \n 8 0          \n 9 0          \n10 0          \n# ... with 775 more rows\n```\n:::\n:::\n\n## Predict probabilities on the testing dataset {.smaller}\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_pred <- predict(email_fit, test_data, type = \"prob\") %>%\n  bind_cols(test_data %>% select(spam, time))\n\nemail_pred\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 785 x 4\n   .pred_0 .pred_1 spam  time               \n     <dbl>   <dbl> <fct> <dttm>             \n 1   0.993 0.00709 0     2012-01-01 11:55:06\n 2   0.998 0.00181 0     2012-01-01 13:38:32\n 3   0.981 0.0191  0     2012-01-01 23:42:16\n 4   0.999 0.00124 0     2012-01-02 09:12:51\n 5   0.988 0.0121  0     2012-01-02 10:45:36\n 6   0.830 0.170   0     2012-01-02 15:55:03\n 7   0.959 0.0410  0     2012-01-02 19:07:17\n 8   0.861 0.139   0     2012-01-02 23:41:35\n 9   0.938 0.0617  0     2012-01-03 10:02:35\n10   0.902 0.0983  0     2012-01-03 05:14:51\n# ... with 775 more rows\n```\n:::\n:::\n\n## A closer look at predictions\n\n:::: {.columns}\n\n::: {.column width=\"80%\"}\n\n::: {.cell highlight.output='[6,10]'}\n\n```{.r .cell-code}\nemail_pred %>%\n  arrange(desc(.pred_1)) %>%\n  print(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 785 x 4\n   .pred_0 .pred_1 spam  time               \n     <dbl>   <dbl> <fct> <dttm>             \n 1  0.0972   0.903 1     2012-02-13 06:15:00\n 2  0.167    0.833 0     2012-01-27 14:05:06\n 3  0.175    0.825 1     2012-02-29 23:40:27\n 4  0.267    0.733 1     2012-03-17 05:13:27\n 5  0.317    0.683 1     2012-03-21 07:33:12\n 6  0.374    0.626 1     2012-02-08 02:00:05\n 7  0.386    0.614 0     2012-01-30 08:20:29\n 8  0.403    0.597 1     2012-01-07 10:11:49\n 9  0.462    0.538 1     2012-03-06 05:46:20\n10  0.463    0.537 0     2012-02-17 16:54:16\n# ... with 775 more rows\n```\n:::\n:::\n:::\n\n::::\n\n## Evaluate the performance {.smaller}\n\n**Receiver operating characteristic (ROC) curve**<sup>+</sup> which plot true positive rate vs. false positive rate (1 - specificity)\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_pred %>%\n  roc_curve(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\"\n  ) %>%\n  autoplot()\n```\n:::\n:::\n\n::: {.column width=\"60%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](mlr_prediction_files/figure-revealjs/unnamed-chunk-30-1.png){width=100%}\n:::\n:::\n:::\n\n::::\n\n::: aside\n<sup>+</sup>Originally developed for operators of military radar receivers, hence the name.\n:::\n\n## Evaluate the performance {.smaller}\n\nFind the area under the curve:\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nemail_pred %>%\n  roc_auc(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\"\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.857\n```\n:::\n:::\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](mlr_prediction_files/figure-revealjs/unnamed-chunk-32-1.png){width=100%}\n:::\n:::\n:::\n\n::::\n\n",
    "supporting": [
      "mlr_prediction_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}